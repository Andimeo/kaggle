{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 different APIs for evaluating the quality of a model's predictions:\n",
    "* **Estimator score method**: Estimators have a ```score``` method provide a default evaluation criterion for the problem they are designed to solve.\n",
    "* **Scoring parameter**: Model-evaluation tools using cross-validation (such as ```model_selection.cross_val_score``` and ```model_selection.GridSearchCV```) rely on an internal *scoring* strategy.\n",
    "* **Metric functions**: The metrics module implements functions assessing prediction error for specific purposes.\n",
    "\n",
    "Finally, Dummy estimators are useful to get a baseline value of those metrics for random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ```scoring``` parameter: defining model evaluation rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection and evaluation using tools, such as ```model_selection.GridSearchCV``` and ```model_selection.cross_val_score```, take a ```scoring``` paramerter that controls what metric they apply to the estimators evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring | Function | Comment\n",
    "--- | --- | ---\n",
    "*Classification* |  | \n",
    "'accuracy' | ```metrics.accuracy_score``` | \n",
    "'average_precision' | ```metrics.average_precision_score``` |\n",
    "'f1' . | ```metrics.f1_score```| for binary targets\n",
    "'f1_micro'| ```metrics.f1_score``` | micro-averaged\n",
    "'f1_macro' | ```metrics.f1_score``` | macro-averaged\n",
    "‘f1_weighted’|\t```metrics.f1_score```|\tweighted average\n",
    "‘f1_samples’|\t```metrics.f1_score```|\tby multilabel sample\n",
    "‘neg_log_loss’|\t```metrics.log_loss```|\trequires predict_proba support\n",
    "‘precision’ etc.|\t```metrics.precision_score```|\tsuffixes apply as with ‘f1’\n",
    "‘recall’ etc.|\t```metrics.recall_score```|\tsuffixes apply as with ‘f1’\n",
    "‘roc_auc’|\t```metrics.roc_auc_score```|\t \n",
    "Clustering| | \t \t \n",
    "‘adjusted_mutual_info_score’|\t```metrics.adjusted_mutual_info_score```|\t \n",
    "‘adjusted_rand_score’|\t```metrics.adjusted_rand_score```|\t \n",
    "‘completeness_score’|\t```metrics.completeness_score```|\t \n",
    "‘fowlkes_mallows_score’|\t```metrics.fowlkes_mallows_score```|\t \n",
    "‘homogeneity_score’|\t```metrics.homogeneity_score```|\t \n",
    "‘mutual_info_score’|\t```metrics.mutual_info_score```|\t \n",
    "‘normalized_mutual_info_score’|\t```metrics.normalized_mutual_info_score```|\t \n",
    "‘v_measure_score’|\t```metrics.v_measure_score```|\t \n",
    "Regression\t \t| | \n",
    "‘explained_variance’|\t```metrics.explained_variance_score```|\t \n",
    "‘neg_mean_absolute_error’|\t```metrics.mean_absolute_error```|\t \n",
    "‘neg_mean_squared_error’|\t```metrics.mean_squared_error```|\t \n",
    "‘neg_mean_squared_log_error’|\t```metrics.mean_squared_log_error```|\t \n",
    "‘neg_median_absolute_error’|\t```metrics.median_absolute_error```|\t \n",
    "‘r2’|\t```metrics.r2_score```|\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07490352, -0.16449405, -0.06685511])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = svm.SVC(probability=True, random_state=0)\n",
    "cross_val_score(clf, X, y, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module ```sklearn.metrics``` also exposes a set of simple functions measuring a prediction error given ground truth and prediction:\n",
    "* functions ending with ```_score``` return a value to maximize, the higher the better\n",
    "* functions ending with ```_error``` or ```_loss``` return a value to minimize, the lower the better.\n",
    "\n",
    "The simplest way to generate a callable object for scoring is by using ```make_scorer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    diff = np.abs(ground_truth - predictions).max()\n",
    "    return np.log(1 + diff)\n",
    "loss = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "\n",
    "ground_truth = [[1], [1]]\n",
    "predictions = [0, 1]\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(ground_truth, predictions)\n",
    "loss(clf, ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(clf, ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 14  9]\n",
      "[1 3 7]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X, y = datasets.make_classification(n_classes=2, random_state=0)\n",
    "svm = LinearSVC(random_state=0)\n",
    "\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "\n",
    "scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn), 'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n",
    "cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)\n",
    "print(cv_results['test_tp'])\n",
    "print(cv_results['test_fn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary classification:\n",
    "\n",
    "[]() |\n",
    "---|---\n",
    "```precision_recall_curve(y_true, probas_pred)```| Compute precision-recall pairs for different probability thresholds\n",
    "```roc_curve(y_true, y_score[, pos_label, ...])``` | Compute Receiver operating characteristic (ROC)\n",
    "\n",
    "multiclass:\n",
    "\n",
    "[]() |\n",
    "---|---\n",
    "```cohen_kappa_score(y1, y2[, labels, weights, ...])```| Cohen's kappa: a statistic that measures inter-annotator agreement\n",
    "```confusion_matrix(y_true, y_pred[, labels, …])``` |\tCompute confusion matrix to evaluate the accuracy of a classification\n",
    "```hinge_loss(y_true, pred_decision[, labels, …])``` |\tAverage hinge loss (non-regularized)\n",
    "```matthews_corrcoef(y_true, y_pred[, …])``` | Compute the Matthews correlation coefficient (MCC)\n",
    "\n",
    "mutlilabel:\n",
    "\n",
    "[]() |\n",
    "---|---\n",
    "```accuracy_score(y_true, y_pred[, normalize, …])```|\tAccuracy classification score.\n",
    "```classification_report(y_true, y_pred[, …])``` |\tBuild a text report showing the main classification metrics\n",
    "```f1_score(y_true, y_pred[, labels, …])``` |\tCompute the F1 score, also known as balanced F-score or F-measure\n",
    "```fbeta_score(y_true, y_pred, beta[, labels, …])``` |\tCompute the F-beta score\n",
    "```hamming_loss(y_true, y_pred[, labels, …])``` |\tCompute the average Hamming loss.\n",
    "```jaccard_similarity_score(y_true, y_pred[, …])``` |\tJaccard similarity coefficient score\n",
    "```log_loss(y_true, y_pred[, eps, normalize, …])``` |\tLog loss, aka logistic loss or cross-entropy loss.\n",
    "```precision_recall_fscore_support(y_true, y_pred)``` |\tCompute precision, recall, F-measure and support for each class\n",
    "```precision_score(y_true, y_pred[, labels, …])``` |\tCompute the precision\n",
    "```recall_score(y_true, y_pred[, labels, …])``` |\tCompute the recall\n",
    "```zero_one_loss(y_true, y_pred[, normalize, …])``` |\tZero-one classification loss.\n",
    "\n",
    "binary class and multilabel:\n",
    "\n",
    "[]() |\n",
    "---|---\n",
    "```average_precision_score(y_true, y_score[, …])```|\tCompute average precision (AP) from prediction scores\n",
    "```roc_auc_score(y_true, y_score[, average, …])```|\tCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ accuracy(y, \\hat{y}) = \\frac{1}{n_{samples}}\\sigma_{i=0}^{n_{samples}-1}1(\\hat{y_i}=y_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multilabel case with binary label indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen's kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4285714285714286"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "By definition, entry $i, j$ in a confusion matrix is the number of observations actually in group $i$, but predicted to be in group $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary problems, we can get counts of true negatives, false positives, false negatives and true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report\n",
    "\n",
    "The ```classification_report``` function builds a text report showing the main classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming loss\n",
    "\n",
    "$$ L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_{labels}}\\sum_{j=0}^{n_{labels}-1}1(\\hat{y_j}\\neq y_j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "hamming_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity coefficient score\n",
    "\n",
    "$$ J(y_i, \\hat{y_i}) = \\frac{|y_i \\cap \\hat{y_i}|}{|y_i \\cup \\hat{y_i}|} $$\n",
    "\n",
    "In binary and multiclass classification, th Jaccard similarity coefficient score is equal to the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "\n",
    "jaccard_similarity_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall and F-measure\n",
    "\n",
    "Intuitively, **precision** is the ability of the classifier not to label as positive a sample that is negative, and **recall** is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "Several functions allow you to analyze the precision, recall and F-measure score:\n",
    "\n",
    "[]() | \n",
    "--- | ---\n",
    "```average_precision_score```(y_true, y_score[, …]) |\tCompute average precision (AP) from prediction scores\n",
    "```f1_score```(y_true, y_pred[, labels, …])\t | Compute the F1 score, also known as balanced F-score or F-measure\n",
    "```fbeta_score```(y_true, y_pred, beta[, labels, …]) |\tCompute the F-beta score\n",
    "```precision_recall_curve```(y_true, probas_pred) |\tCompute precision-recall pairs for different probability thresholds\n",
    "```precision_recall_fscore_support```(y_true, y_pred) |\tCompute precision, recall, F-measure and support for each class\n",
    "```precision_score```(y_true, y_pred[, labels, …])\t| Compute the precision\n",
    "```recall_score```(y_true, y_pred[, labels, …])\t| Compute the recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ precision = \\frac{tp}{tp + fp} $$\n",
    "$$ recall = \\frac{tp}{tp + fn} $$\n",
    "$$ F_\\beta = (1+\\beta^2)\\frac{precision \\times recall}{\\beta^2precision + recall} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.fbeta_score(y_true, y_pred, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.5       , 1.        , 1.        ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.5, 0.5, 0. ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35, 0.4 , 0.8 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```average_precision_score``` function computes the **average precision** (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as\n",
    "\n",
    "$$ AP = \\sum_n(R_n-R_{n-1})P_n $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiclass and multilabel classification task, the notions of precision, recall and F-measures can be applied to each label independently.\n",
    "\n",
    "average | Precision | Recall | F_beta\n",
    "--- | --- | --- | ---\n",
    "\"micro\" | $P(y, \\hat{y})$ | $R(y, \\hat{y})$ | $F_\\beta(y, \\hat{y})$\n",
    "\"samples\" | $\\frac{1}{\\lvert S \\rvert}\\sum_{s\\in S}P(y_s, \\hat{y_s})$ | $\\frac{1}{\\lvert S \\rvert}\\sum_{s\\in S}R(y_s, \\hat{y_s})$ |$\\frac{1}{\\lvert S \\rvert}\\sum_{s\\in S}F_\\beta(y_s, \\hat{y_s})$ \n",
    "\"macro\" | $\\frac{1}{\\lvert L \\rvert}\\sum_{l\\in L}P(y_l, \\hat{y_l})$ | $\\frac{1}{\\lvert L \\rvert}\\sum_{l\\in L}R(y_l, \\hat{y_l})$ |$\\frac{1}{\\lvert L \\rvert}\\sum_{l\\in L}F_\\beta(y_l, \\hat{y_l})$\n",
    "\"weighted\" | $\\frac{1}{\\sum_{l\\in L}\\lvert \\hat{y_l} \\rvert}\\sum_{l\\in L}\\lvert \\hat{y_l} \\rvert R(y_l, \\hat{y_l})$ |$\\frac{1}{\\sum_{l\\in L}\\lvert \\hat{y_l} \\rvert}\\sum_{l\\in L}\\lvert \\hat{y_l} \\rvert P(y_l, \\hat{y_l})$ |$\\frac{1}{\\sum_{l\\in L}\\lvert \\hat{y_l} \\rvert}\\sum_{l\\in L}\\lvert \\hat{y_l} \\rvert P(y_l, \\hat{y_l})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "metrics.precision_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23809523809523805"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.66666667, 0.        , 0.        ]),\n",
       " array([1., 0., 0.]),\n",
       " array([0.71428571, 0.        , 0.        ]),\n",
       " array([2, 2, 2]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass classification with a \"negative class\", it is possible to exclude some labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, labels not present in the data sample may be accounted for in macro-averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss\n",
    "\n",
    "If the labels are encoded with +1 and -1, $y$ is the true value, and $w$ is the predicted decisions as output by ```decision_function```, then the hinge loss is defined as:\n",
    "\n",
    "$$ L_{Hinge}(y, w) = max\\{1-wy, 0\\} = \\lvert 1-wy \\rvert_+ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import hinge_loss\n",
    "X = [[0], [1]]\n",
    "y = [-1, 1]\n",
    "est = svm.LinearSVC(random_state=0)\n",
    "est.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.18173682,  2.36360149,  0.09093234])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
    "pred_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30302255420413554"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinge_loss([-1, 1, 1], pred_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log loss\n",
    "\n",
    "Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regressin and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (```predict_proba```) of a classifier of its discrete predictions.\n",
    "\n",
    "For binary classification with a true label $y \\in \\{0, 1\\}$ and a probability estimate $p = Pr(y=1)$, the log loss per sample is the negative log-likelihood of the classifier given the true label:\n",
    "\n",
    "$$ L_{log}(y, p) = -logPr(y|p) = -(ylog(p) + (1-y)log(1-p)) $$\n",
    "\n",
    "This extends to the multiclass case as follows.\n",
    "\n",
    "$$ L_{log}(Y, P) = -logPr(Y|P) = -\\frac{1}{N}\\sum_{i=0}^{N-1}\\sum_{k=0}^{K-1}y_{i,k}log{p_{i,k}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1738073366910675"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matthews correlation coefficient\n",
    "\n",
    "$$ MCC = \\frac{tp\\times tn - fp \\times fn}{\\sqrt{(tp+fp)(tp+fn)(tn+fp)(tn+fn)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3333333333333333"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "y_true = [1, 1, 1, -1]\n",
    "y_pred = [1, -1, 1, 1]\n",
    "matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver operating characteristic (ROC)\n",
    "\n",
    "> “A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 0.5, 1. ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 1. , 1. ])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.4 , 0.35, 0.1 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```roc_auc_score``` function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero one loss\n",
    "\n",
    "$$ L_{0-1}(y_i, \\hat{y_i}) = 1(\\hat{y_i} \\neq y_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "zero_one_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_one_loss(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier score loss\n",
    "\n",
    "$$ BS = \\frac{1}{N}\\sum_{t=1}^{N}(f_t - o_t)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import brier_score_loss\n",
    "y_true = np.array([0, 1, 1, 0])\n",
    "y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
    "y_prob = np.array([0.1, 0.9, 0.8, 0.4])\n",
    "y_pred = np.array([0, 1, 1, 0])\n",
    "brier_score_loss(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score_loss(y_true, 1-y_prob, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score_loss(y_true, y_prob > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel ranking metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage error\n",
    "\n",
    "$$ coverage(y, \\hat{f}) = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}max_{j:y_{i,j}=1}rank_{i,j} $$\n",
    "\n",
    "with $$ rank_{i, j} = \\lvert \\{ k:\\hat{f_{ik}} \\ge \\hat{f_{ij}} \\} \\rvert$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import coverage_error\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "coverage_error(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label ranking average precision\n",
    "\n",
    "$$ LRAP(y, \\hat{f}) = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}\\frac{1}{\\lvert y_i \\rvert}\\sum_{j:y_{ij=1}}\\frac{\\lvert \\mathcal{L_{ij}} \\rvert}{rank_{ij}} $$\n",
    "\n",
    "with $$ \\mathcal{L_{ij}} = \\{ k:y_{ik}=1, \\hat{f_{ik}} \\ge \\hat{f_{ij}} \\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41666666666666663"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "label_ranking_average_precision_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking loss\n",
    "\n",
    "$$ ranking\\_loss(y, \\hat{f})=\\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}\\frac{1}{\\lvert y_i \\rvert(n_{labels}-\\lvert y_i \\rvert)}\\lvert \\{ (k, l): \\hat{f_{ik}} < \\hat{f_{il}}, y_{ik}=1, y_{il}=0 \\} \\rvert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "label_ranking_loss(y_true, y_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With the following prediction, we have perfect and minimal loss\n",
    "y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\n",
    "label_ranking_loss(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression metrics\n",
    "\n",
    "The ```sklearn.metrics``` module implements several loss, score and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: ```mean_squared_error```, ```mean_absolute_error```, ```explained_variance_score``` and ```r2_score```.\n",
    "\n",
    "These functions have an ```multioutput``` keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is ```'uniform_average'```, which specifies a uniformly weighted mean over outputs. If an ```ndarray``` of shape ```(n_outputs,)``` is passed, then its entries are interpreted as weights and an according weighted average is returned. If ```multioutput``` is ```'raw_values'``` is specified, then all unaltered individual scores or losses will be returned in an array of shape ```(n_outputs,)```.\n",
    "\n",
    "The ```r2_score``` and ```explained_variance_score``` accept an additional value ```'variance_weighted'``` for the ```multioutput``` parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained variance score\n",
    "\n",
    "$$ explained\\_variance(y, \\hat{y}) = 1 - \\frac{Var\\{y-\\hat{y}\\}}{Var\\{y\\}} $$\n",
    "The best possible score is 1.0, lower values are worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9571734475374732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95717345])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(explained_variance_score(y_true, y_pred))\n",
    "explained_variance_score(y_true, y_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96774194 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9903225806451612"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(explained_variance_score(y_true, y_pred, multioutput='raw_values'))\n",
    "explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error\n",
    "\n",
    "$$ MAE(y, \\hat{y}) = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}\\lvert y_i - \\hat{y_i} \\rvert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_true, y_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error\n",
    "\n",
    "$$ MSE(y, \\hat{y}) = \\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}(y_i - \\hat{y_i})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "mean_squared_error(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared logarithmic error\n",
    "\n",
    "$$ MSLE(y, \\hat{y})=\\frac{1}{n_{samples}}\\sum_{i=0}^{n_{samples}-1}(log_e(1+y_i)-log_e(1+\\hat{y_i}))^2 $$\n",
    "\n",
    "This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03973012298459379"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "y_true = [3, 5, 2.5, 7]\n",
    "y_pred = [2.5, 5, 4, 8]\n",
    "mean_squared_log_error(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044199361889160536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
    "y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
    "mean_squared_log_error(y_true, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median absolute error\n",
    "\n",
    "$$ MedAE(y, \\hat{y})=median(\\lvert y_1-\\hat{y_1} \\rvert, ..., \\lvert y_n - \\hat{y_n} \\rvert) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "median_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ score, the coefficient of determination\n",
    "\n",
    "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{samples}-1}(y_i-\\hat{y_i})^2}{\\sum_{i=0}^{n_{samples}-1}(y_i-\\bar{y_i})^2} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486081370449679"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382566585956417"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_score(y_true, y_pred, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368005266622779"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "r2_score(y_true, y_pred, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96543779, 0.90816327])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253456221198156"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true, y_pred, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy estimators\n",
    "\n",
    "```DummyClassifier``` implements several such simple strategies for classification:\n",
    "* ```stratiffied``` generates random predictions by respecting the training set class distribution.\n",
    "* ```most_frequent``` always predicts the most frequent label in the training set.\n",
    "* ```prior``` always predicts the class that maximizes the class prior.\n",
    "* ```uniform``` generates predictions uniformly at random.\n",
    "* ```constant``` always predicts a constant label that is provided by the user.\n",
    "\n",
    "Note that with all these strategies, the ```predict``` method completely ignores the input data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y[y != 1] = -1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631578947368421"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```DummyRegressor``` also implemetns four simple rules of thumb for regression:\n",
    "* ```mean``` always predicts the mean of the training targets.\n",
    "* ```median``` always predicts the median of the training targets.\n",
    "* ```quantile``` always predicts a specified quantile of the training set, provided with the quantile parameter.\n",
    "* ```constant``` always predicts a constant value that is provided by the user.\n",
    "\n",
    "In all these strategies, the ```predict``` method completely ignores the input data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
